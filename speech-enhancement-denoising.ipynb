{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>datasets paths</h1>\n<h3>https://www.kaggle.com/chrisfilo/urbansound8k</h3>\n<h3>https://www.kaggle.com/mathurinache/the-lj-speech-dataset</h3>","metadata":{}},{"cell_type":"code","source":"import librosa\nimport librosa.display\nimport IPython as ip\nimport os\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport os\nimport pickle\n\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, Flatten, Dense, Reshape, Conv2DTranspose, Activation, Lambda\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import MeanSquaredError\nimport numpy as np\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2021-10-13T23:42:57.678910Z","iopub.execute_input":"2021-10-13T23:42:57.680068Z","iopub.status.idle":"2021-10-13T23:43:03.784720Z","shell.execute_reply.started":"2021-10-13T23:42:57.679840Z","shell.execute_reply":"2021-10-13T23:43:03.783870Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"<h3>for this i will use a vae (variational auto encoder) and train it to get as input an audio containing a noise from the urbain dataset and then use the spectrogram of this noisy audio to generate a new spectrogram of the audio without the noise for this we need to get a voice speaking dataset and mixe it with the noise so how could we mix 2 audio files in librosa?</h3>","metadata":{}},{"cell_type":"code","source":"# here we read both audios and combine them in a one audio using librosa\naudio1 = '../input/the-lj-speech-dataset/LJSpeech-1.1/wavs/LJ001-0004.wav'\naudio2= '../input/urbansound8k/fold4/115415-9-0-6.wav'\n# the audios must have the same durations\naud1, sample_rate1 = librosa.load(audio1, mono=True, duration=2, sr=16384)\naud2, sample_rate2 = librosa.load(audio2, mono=True, duration=2, sr=16384)\n\n# MERGE\ncombined = (aud1+aud2)/2\ncombined_sample_rate = int((sample_rate1+sample_rate2)/2)\n\n# DISPLAY\nprint(\"the mixed audio\")\nlibrosa.display.waveplot(combined, sr=combined_sample_rate)\nip.display.Audio(combined, rate=combined_sample_rate)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T23:43:03.786485Z","iopub.execute_input":"2021-10-13T23:43:03.786793Z","iopub.status.idle":"2021-10-13T23:43:04.917078Z","shell.execute_reply.started":"2021-10-13T23:43:03.786753Z","shell.execute_reply":"2021-10-13T23:43:04.916301Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(\"the original audio\")\nlibrosa.display.waveplot(aud1, sr=sample_rate1)\nip.display.Audio(aud1, rate=sample_rate1)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T23:43:04.918537Z","iopub.execute_input":"2021-10-13T23:43:04.918835Z","iopub.status.idle":"2021-10-13T23:43:05.219652Z","shell.execute_reply.started":"2021-10-13T23:43:04.918796Z","shell.execute_reply":"2021-10-13T23:43:05.218926Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"combined.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-13T23:43:05.221661Z","iopub.execute_input":"2021-10-13T23:43:05.222004Z","iopub.status.idle":"2021-10-13T23:43:05.227326Z","shell.execute_reply.started":"2021-10-13T23:43:05.221964Z","shell.execute_reply":"2021-10-13T23:43:05.226651Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<h1>now lets start preparing the data</h1>\n<h3>we will take 10000 human voice and mix it with 1 random sound from the 10 folders in urbain sounds so we will have around 10000 sample</h3>","metadata":{}},{"cell_type":"code","source":"# the nice thing with librosa is that we don't need to normelize it does it for us\n# def normalise(array):\n#     norm_array = (array - array.min()) / (array.max() - array.min())\n#     return norm_array, (array.min(), array.max())\n\n# def denormalise(array, original_min, original_max):\n#     array = array * (original_max - original_min) + original_min\n#     return array","metadata":{"execution":{"iopub.status.busy":"2021-10-13T23:43:05.228720Z","iopub.execute_input":"2021-10-13T23:43:05.229191Z","iopub.status.idle":"2021-10-13T23:43:05.235366Z","shell.execute_reply.started":"2021-10-13T23:43:05.229153Z","shell.execute_reply":"2021-10-13T23:43:05.234601Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# here i tried with log spectrogram but it didn't really work well\n# def extract_log_spectrograms(human_wave_path, urban_wave_path):\n#     n_fft = 512\n#     hop_length = 256\n#     aud1, sample_rate1 = librosa.load(human_wave_path, mono=True, duration=0.74)\n#     aud2, sample_rate2 = librosa.load(urban_wave_path, mono=True, duration=0.74)\n    \n#     # MERGE\n#     combined = (aud1+aud2)/2\n#     combined_sample_rate = int((sample_rate1+sample_rate2)/2)\n    \n#     # extract log spectrogram\n#     stft = librosa.stft(combined, n_fft=n_fft, hop_length=hop_length)\n#     spectrogram = np.abs(stft)\n#     x_log_spectrogram = librosa.amplitude_to_db(spectrogram)\n#     x_log_spectrogram, min_max = normalise(x_log_spectrogram)\n    \n#     stft = librosa.stft(aud1, n_fft=n_fft, hop_length=hop_length)\n#     spectrogram = np.abs(stft)\n#     y_log_spectrogram = librosa.amplitude_to_db(spectrogram)\n#     y_log_spectrogram, _ = normalise(y_log_spectrogram)\n        \n#     return x_log_spectrogram, y_log_spectrogram, min_max","metadata":{"execution":{"iopub.status.busy":"2021-10-13T23:43:05.237000Z","iopub.execute_input":"2021-10-13T23:43:05.237268Z","iopub.status.idle":"2021-10-13T23:43:05.244167Z","shell.execute_reply.started":"2021-10-13T23:43:05.237234Z","shell.execute_reply":"2021-10-13T23:43:05.243423Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def extract_wave(human_wave_path, urban_wave_path):\n    aud1, sample_rate1 = librosa.load(human_wave_path, mono=True, duration=2, sr=16384)\n    aud2, sample_rate2 = librosa.load(urban_wave_path, mono=True, duration=2, sr=16384)\n    \n    # MERGE\n    combined = (aud1+aud2)/2\n    combined_sample_rate = int((sample_rate1+sample_rate2)/2)\n        \n    return combined, aud1, combined_sample_rate","metadata":{"execution":{"iopub.status.busy":"2021-10-13T23:43:05.245443Z","iopub.execute_input":"2021-10-13T23:43:05.245821Z","iopub.status.idle":"2021-10-13T23:43:05.256624Z","shell.execute_reply.started":"2021-10-13T23:43:05.245786Z","shell.execute_reply":"2021-10-13T23:43:05.255965Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# here we are going to use only the aire conditionning sound\nurban_csv = pd.read_csv(\"../input/urbansound8k/UrbanSound8K.csv\")\nurban_csv = urban_csv[urban_csv[\"class\"] == \"air_conditioner\"]\nurban_csv = urban_csv[urban_csv[\"fold\"] == 5]\nurban_csv","metadata":{"execution":{"iopub.status.busy":"2021-10-13T23:43:05.258129Z","iopub.execute_input":"2021-10-13T23:43:05.258396Z","iopub.status.idle":"2021-10-13T23:43:05.335065Z","shell.execute_reply.started":"2021-10-13T23:43:05.258362Z","shell.execute_reply":"2021-10-13T23:43:05.334387Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"air_conditionning_folders = urban_csv[\"slice_file_name\"]","metadata":{"execution":{"iopub.status.busy":"2021-10-13T23:43:05.336201Z","iopub.execute_input":"2021-10-13T23:43:05.336459Z","iopub.status.idle":"2021-10-13T23:43:05.341600Z","shell.execute_reply.started":"2021-10-13T23:43:05.336415Z","shell.execute_reply":"2021-10-13T23:43:05.340834Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"x_train, y_train = [],[]\ndef data_pipeline(human_voices_folder,urban_sounds):\n    human_voice_waves = os.listdir(human_voices_folder)\n    for i in tqdm(range(8500)):\n        human_voice_wave = human_voice_waves[i]\n        rand_sampled_urban = np.random.choice(air_conditionning_folders)\n        try:\n            urban_path = os.path.join(str(urban_sounds),str(rand_sampled_urban))\n            human_path = os.path.join(str(human_voices_folder),str(human_voice_wave))\n            x, y, _ = extract_wave(human_path, urban_path)\n            x_train.append(x)\n            y_train.append(y)\n        except:\n            pass\n                \n    return np.array(x_train), np.array(y_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T23:43:05.345228Z","iopub.execute_input":"2021-10-13T23:43:05.345922Z","iopub.status.idle":"2021-10-13T23:43:05.353320Z","shell.execute_reply.started":"2021-10-13T23:43:05.345884Z","shell.execute_reply":"2021-10-13T23:43:05.352306Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"human_voices_folder= \"../input/the-lj-speech-dataset/LJSpeech-1.1/wavs\"\nurban_sounds = \"../input/urbansound8k/fold5\"\nx, y = data_pipeline(human_voices_folder,urban_sounds)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T23:43:05.354666Z","iopub.execute_input":"2021-10-13T23:43:05.355003Z","iopub.status.idle":"2021-10-14T00:01:45.541183Z","shell.execute_reply.started":"2021-10-13T23:43:05.354968Z","shell.execute_reply":"2021-10-14T00:01:45.540333Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"x.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-14T00:01:45.542742Z","iopub.execute_input":"2021-10-14T00:01:45.543118Z","iopub.status.idle":"2021-10-14T00:01:45.550087Z","shell.execute_reply.started":"2021-10-14T00:01:45.543076Z","shell.execute_reply":"2021-10-14T00:01:45.549122Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# we reshape for the un conv layer\nx = x.reshape(-1,x.shape[1], 1)\ny = y.reshape(-1,y.shape[1], 1)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T00:01:45.551658Z","iopub.execute_input":"2021-10-14T00:01:45.551926Z","iopub.status.idle":"2021-10-14T00:01:45.560783Z","shell.execute_reply.started":"2021-10-14T00:01:45.551884Z","shell.execute_reply":"2021-10-14T00:01:45.560042Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"<h1>the autoencoder</h1>","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential()\n\nmodel.add(tf.keras.layers.Conv1D(filters=32, strides=32, input_shape=(16384*2,1), padding=\"same\", kernel_size=2048, use_bias=True))\n# model.add(tf.keras.layers.Conv1D(filters=64, strides=64, padding=\"same\", kernel_size=2048, use_bias=True))\nmodel.add(Activation(\"tanh\"))\n# model.add(tf.keras.layers.Conv1DTranspose(filters=32, strides=64, padding=\"same\", kernel_size=2048, use_bias=True))\nmodel.add(tf.keras.layers.Conv1DTranspose(filters=1, strides=32, padding=\"same\", kernel_size=2048, use_bias=True))\nmodel.add(Activation(\"tanh\"))\n\nmodel.compile(optimizer=\"adam\",loss=\"mse\")","metadata":{"execution":{"iopub.status.busy":"2021-10-14T00:24:29.358535Z","iopub.execute_input":"2021-10-14T00:24:29.359223Z","iopub.status.idle":"2021-10-14T00:24:29.414076Z","shell.execute_reply.started":"2021-10-14T00:24:29.359176Z","shell.execute_reply":"2021-10-14T00:24:29.413362Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-14T00:24:30.446382Z","iopub.execute_input":"2021-10-14T00:24:30.447039Z","iopub.status.idle":"2021-10-14T00:24:30.457802Z","shell.execute_reply.started":"2021-10-14T00:24:30.446996Z","shell.execute_reply":"2021-10-14T00:24:30.456855Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"model.fit(x, y, epochs=100)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T00:24:32.437069Z","iopub.execute_input":"2021-10-14T00:24:32.438053Z","iopub.status.idle":"2021-10-14T00:29:49.834336Z","shell.execute_reply.started":"2021-10-14T00:24:32.437999Z","shell.execute_reply":"2021-10-14T00:29:49.833635Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# lets try it\nhuman_wave_path = \"../input/the-lj-speech-dataset/LJSpeech-1.1/wavs/LJ001-0005.wav\"\nurban_wave_path = \"../input/urbansound8k/fold5/100852-0-0-6.wav\"\nx_wave, y_wave, sr = extract_wave(human_wave_path, urban_wave_path)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T00:31:06.469939Z","iopub.execute_input":"2021-10-14T00:31:06.470599Z","iopub.status.idle":"2021-10-14T00:31:06.607601Z","shell.execute_reply.started":"2021-10-14T00:31:06.470545Z","shell.execute_reply":"2021-10-14T00:31:06.606877Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"x_wave = x_wave.reshape(1,x.shape[1],1)\nx_wave.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-14T00:31:06.748134Z","iopub.execute_input":"2021-10-14T00:31:06.748354Z","iopub.status.idle":"2021-10-14T00:31:06.755032Z","shell.execute_reply.started":"2021-10-14T00:31:06.748329Z","shell.execute_reply":"2021-10-14T00:31:06.753989Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(x_wave)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T00:31:06.998225Z","iopub.execute_input":"2021-10-14T00:31:06.998479Z","iopub.status.idle":"2021-10-14T00:31:07.043741Z","shell.execute_reply.started":"2021-10-14T00:31:06.998452Z","shell.execute_reply":"2021-10-14T00:31:07.042722Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"y_pred = y_pred.reshape(x.shape[1])","metadata":{"execution":{"iopub.status.busy":"2021-10-14T00:31:07.239167Z","iopub.execute_input":"2021-10-14T00:31:07.239395Z","iopub.status.idle":"2021-10-14T00:31:07.243876Z","shell.execute_reply.started":"2021-10-14T00:31:07.239369Z","shell.execute_reply":"2021-10-14T00:31:07.242595Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"librosa.display.waveplot(y_pred , sr=16384)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T00:31:07.796390Z","iopub.execute_input":"2021-10-14T00:31:07.796890Z","iopub.status.idle":"2021-10-14T00:31:08.174429Z","shell.execute_reply.started":"2021-10-14T00:31:07.796851Z","shell.execute_reply":"2021-10-14T00:31:08.173541Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"librosa.display.waveplot(y_wave , sr=16384)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T00:31:08.516119Z","iopub.execute_input":"2021-10-14T00:31:08.516364Z","iopub.status.idle":"2021-10-14T00:31:08.822255Z","shell.execute_reply.started":"2021-10-14T00:31:08.516336Z","shell.execute_reply":"2021-10-14T00:31:08.821564Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"librosa.display.waveplot(x_wave.reshape(x.shape[1]) , sr=16384)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T00:31:09.908701Z","iopub.execute_input":"2021-10-14T00:31:09.909362Z","iopub.status.idle":"2021-10-14T00:31:10.366152Z","shell.execute_reply.started":"2021-10-14T00:31:09.909323Z","shell.execute_reply":"2021-10-14T00:31:10.365452Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"ip.display.Audio(y_pred , rate=16384)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T00:31:11.596973Z","iopub.execute_input":"2021-10-14T00:31:11.597534Z","iopub.status.idle":"2021-10-14T00:31:11.609270Z","shell.execute_reply.started":"2021-10-14T00:31:11.597486Z","shell.execute_reply":"2021-10-14T00:31:11.608517Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"ip.display.Audio(x_wave.reshape(x.shape[1]) , rate=16384)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T00:31:12.397526Z","iopub.execute_input":"2021-10-14T00:31:12.398265Z","iopub.status.idle":"2021-10-14T00:31:12.406983Z","shell.execute_reply.started":"2021-10-14T00:31:12.398227Z","shell.execute_reply":"2021-10-14T00:31:12.406121Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"# actually its working good the speech is more clear","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}